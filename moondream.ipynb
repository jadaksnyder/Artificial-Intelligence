{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LjkNbYmPQ92X"
      },
      "source": [
        "# Moondream\n",
        "#### A tiny open-source computer-vision model that runs everywhere and kicks ass (cit.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ihufKGisQ92Y"
      },
      "source": [
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/antonio-f/Moondream/blob/main/moondream.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xpi8sIdLQ92Y"
      },
      "source": [
        "Moondream is a lightweight and fast transformer-based model that can be used for a variety of computer vision tasks. \"moondream2\" is a 1.86B parameter model. In essence, the model is a Vision Transformer ([ViT](https://en.wikipedia.org/wiki/Vision_transformer)) together with a language model that is capable of generating human-like text based on the input visual information. This allows the model to perform tasks such as image captioning, visual question answering, and multimodal reasoning. Our aim is to explore the Moondream model in the simplest way."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFyBzebQQ92Y"
      },
      "source": [
        "Although possible, there is no real need to clone the Moondream GitHub [repository](https://github.com/vikhyat/moondream), instead there are two quick choices:\n",
        "\n",
        "1) run the notebook using Google Colab (avoiding installing locally) or\n",
        "2) open a Jupyter notebook from an environment with `pip` and PyTorch and run the following cells.\n",
        "\n",
        "There is a third option, that is, testing directly the model from this [demo](https://huggingface.co/spaces/vikhyatk/moondream2) page without following the content of this notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJK6etEJQ92Y"
      },
      "source": [
        "We can leverage the Hugging Face `transformers` library to build powerful natural language processing models.\n",
        "The `transformers` library provides a wide range of pre-trained models for various NLP tasks, such as text classification, named entity recognition, and language generation. `einops` library is useful to transform tensors into the desired shape for the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5qJnno20jJ_K"
      },
      "outputs": [],
      "source": [
        "! pip install transformers einops"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8hCpLkf2Q92Z"
      },
      "source": [
        "The model is updated regularly, so it is recommended pinning the model version to a specific release."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "49CKVsArjQaa"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from PIL import Image\n",
        "\n",
        "model_id = \"vikhyatk/moondream2\"\n",
        "revision = \"2024-05-08\"\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id, trust_remote_code=True, revision=revision\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_GJ7c43Q92Z"
      },
      "source": [
        "Enabling Flash Attention by appropriately setting the parameters: this is shown in the cell below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g2AT7wDkQ92Z"
      },
      "outputs": [],
      "source": [
        "# Uncomment the following to enable Flash Attention on the text model\n",
        "\n",
        "# model = AutoModelForCausalLM.from_pretrained(\n",
        "#     model_id, trust_remote_code=True, revision=revision,\n",
        "#     torch_dtype=torch.float16, attn_implementation=\"flash_attention_2\"\n",
        "# ).to(\"cuda\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KFNxUykKQ92Z"
      },
      "source": [
        "Loading the tokenizer for the specified model and revision."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MYLkDP07Q92Z",
        "outputId": "5d861495-f1c6-4edc-f39a-0232b9a60053"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_id, revision=revision)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6CoCQQu3Q92a"
      },
      "source": [
        "The following cell is to enable URL image retrieval. However, we can also use paths to files as described later. Click on the link to see the image fed to the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "My7LvOb4m5Ju"
      },
      "outputs": [],
      "source": [
        "# This is for opening images from URLs\n",
        "import requests\n",
        "from io import BytesIO\n",
        "\n",
        "response = requests.get(\"https://raw.githubusercontent.com/vikhyat/moondream/main/assets/demo-2.jpg\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VOPr8vWBQ92a"
      },
      "source": [
        "The following cell creates a PIL Image object from an image file or an image URL. Uncomment to make a choice."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c--BWGfmQ92a"
      },
      "outputs": [],
      "source": [
        "# image = Image.open('<IMAGE_PATH>') # image from file\n",
        "image = Image.open(BytesIO(response.content)) # image from URL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-yTL86MQ92a"
      },
      "source": [
        "Now we encode the input image using the model's image encoding function, then print the answer to the question."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q6v6S4SRjsii",
        "outputId": "c0692f09-96d7-42d3-8291-e0db31db21d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The image shows a black computer server rack with three levels, each containing multiple computer components. The topmost level has two computer monitors, one with a blue screen and the other with a green screen. The middle level has two computer fans, one with a blue fan and the other with a green fan. The bottom level has two computer mice, one with a blue mouse and the other with a green mouse. The rack is placed on a carpeted floor, and a brick wall is visible in the background.\n"
          ]
        }
      ],
      "source": [
        "enc_image = model.encode_image(image)\n",
        "print(model.answer_question(enc_image, \"Describe this image.\", tokenizer))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wHGsws16Q92b"
      },
      "source": [
        "Batch inference is also supported."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8XvXpr9QQ92b"
      },
      "outputs": [],
      "source": [
        "# Uncomment below to use\n",
        "\n",
        "# answers = moondream.batch_answer(\n",
        "#     images=[Image.open('<IMAGE_PATH_1>'), Image.open('<IMAGE_PATH_2>')],\n",
        "#     prompts=[\"Describe this image.\", \"Are there people in this image?\"],\n",
        "#     tokenizer=tokenizer,\n",
        "# )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8B8NT7XQ92b"
      },
      "source": [
        "### Limitations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HVCE3GzoQ92b"
      },
      "source": [
        "The following are taken verbatim from the repo page.\n",
        "\n",
        "\n",
        "- The model may generate inaccurate statements, and struggle to understand intricate or nuanced instructions.<br><br>\n",
        "- The model may not be free from societal biases. Users should be aware of this and exercise caution and critical thinking when using the model.<br><br>\n",
        "- The model may generate offensive, inappropriate, or hurtful content if it is prompted to do so.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nWKuF8_iQ92b"
      },
      "source": [
        "### Useful links"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qd2dgTobQ92b"
      },
      "source": [
        "Moondream GitHub [repository](https://github.com/vikhyat/moondream) <br><br>\n",
        "Moondream Live [Demo](https://huggingface.co/spaces/vikhyatk/moondream2) <br><br>\n",
        "Hugging Face [page](https://huggingface.co/vikhyatk/moondream2) <br><br>\n",
        "Vision Transformer ([ViT](https://en.wikipedia.org/wiki/Vision_transformer))\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}